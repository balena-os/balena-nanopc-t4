From 65a8d8f5acd3cdd6e0ce51fed3f35f976174c9c4 Mon Sep 17 00:00:00 2001
From: Florin Sarbu <florin@balena.io>
Date: Thu, 8 Aug 2019 13:14:42 +0200
Subject: [PATCH] Fixes for compiling Mali kernel driver release r14p0-01rel0 
 against kernel version 5.2.0

Upstream-Status: Pending
Signed-off-by: Florin Sarbu <florin@balena.io>
---
 drivers/gpu/arm/midgard/mali_kbase_defs.h      |  2 +-
 drivers/gpu/arm/midgard/mali_kbase_dma_fence.c | 50 +++++++++++++-------------
 drivers/gpu/arm/midgard/mali_kbase_dma_fence.h |  6 ++--
 drivers/gpu/arm/midgard/mali_kbase_jd.c        |  4 +--
 drivers/gpu/arm/midgard/mali_kbase_mem.c       | 12 ++-----
 drivers/gpu/arm/midgard/mali_kbase_mem_linux.c | 12 +++----
 6 files changed, 38 insertions(+), 48 deletions(-)

diff --git a/drivers/gpu/arm/midgard/mali_kbase_defs.h b/drivers/gpu/arm/midgard/mali_kbase_defs.h
index df1a6f0..fcb1591 100755
--- a/drivers/gpu/arm/midgard/mali_kbase_defs.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_defs.h
@@ -447,7 +447,7 @@ struct kbase_jd_atom {
 		 * regardless of the event_code of the katom (signal also on
 		 * failure).
 		 */
-		struct fence *fence;
+		struct dma_fence *fence;
 		/* The dma-buf fence context number for this atom. A unique
 		 * context number is allocated to each katom in the context on
 		 * context creation.
diff --git a/drivers/gpu/arm/midgard/mali_kbase_dma_fence.c b/drivers/gpu/arm/midgard/mali_kbase_dma_fence.c
index 9292f81..e28740f 100755
--- a/drivers/gpu/arm/midgard/mali_kbase_dma_fence.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_dma_fence.c
@@ -22,7 +22,7 @@
 #include "mali_kbase_dma_fence.h"
 
 #include <linux/atomic.h>
-#include <linux/fence.h>
+#include <linux/dma-fence.h>
 #include <linux/list.h>
 #include <linux/lockdep.h>
 #include <linux/mutex.h>
@@ -56,19 +56,19 @@ kbase_dma_fence_waiters_remove(struct kbase_jd_atom *katom)
 }
 
 static const char *
-kbase_dma_fence_get_driver_name(struct fence *fence)
+kbase_dma_fence_get_driver_name(struct dma_fence *fence)
 {
 	return kbase_drv_name;
 }
 
 static const char *
-kbase_dma_fence_get_timeline_name(struct fence *fence)
+kbase_dma_fence_get_timeline_name(struct dma_fence *fence)
 {
 	return kbase_timeline_name;
 }
 
 static bool
-kbase_dma_fence_enable_signaling(struct fence *fence)
+kbase_dma_fence_enable_signaling(struct dma_fence *fence)
 {
 	/* If in the future we need to add code here remember to
 	 * to get a reference to the fence and release it when signaling
@@ -78,30 +78,30 @@ kbase_dma_fence_enable_signaling(struct fence *fence)
 }
 
 static void
-kbase_dma_fence_fence_value_str(struct fence *fence, char *str, int size)
+kbase_dma_fence_fence_value_str(struct dma_fence *fence, char *str, int size)
 {
 	snprintf(str, size, "%u", fence->seqno);
 }
 
-static const struct fence_ops kbase_dma_fence_ops = {
+static const struct dma_fence_ops kbase_dma_fence_ops = {
 	.get_driver_name = kbase_dma_fence_get_driver_name,
 	.get_timeline_name = kbase_dma_fence_get_timeline_name,
 	.enable_signaling = kbase_dma_fence_enable_signaling,
 	/* Use the default wait */
-	.wait = fence_default_wait,
+	.wait = dma_fence_default_wait,
 	.fence_value_str = kbase_dma_fence_fence_value_str,
 };
 
-static struct fence *
+static struct dma_fence *
 kbase_dma_fence_new(unsigned int context, unsigned int seqno)
 {
-	struct fence *fence;
+	struct dma_fence *fence;
 
 	fence = kzalloc(sizeof(*fence), GFP_KERNEL);
 	if (!fence)
 		return NULL;
 
-	fence_init(fence,
+	dma_fence_init(fence,
 		   &kbase_dma_fence_ops,
 		   &kbase_dma_fence_lock,
 		   context,
@@ -212,7 +212,7 @@ kbase_dma_fence_free_callbacks(struct kbase_jd_atom *katom, bool queue_worker)
 		bool ret;
 
 		/* Cancel callbacks that hasn't been called yet. */
-		ret = fence_remove_callback(cb->fence, &cb->fence_cb);
+		ret = dma_fence_remove_callback(cb->fence, &cb->fence_cb);
 		if (ret) {
 			int ret;
 
@@ -235,7 +235,7 @@ kbase_dma_fence_free_callbacks(struct kbase_jd_atom *katom, bool queue_worker)
 		 * Release the reference taken in
 		 * kbase_dma_fence_add_callback().
 		 */
-		fence_put(cb->fence);
+		dma_fence_put(cb->fence);
 		list_del(&cb->node);
 		kfree(cb);
 	}
@@ -329,8 +329,8 @@ kbase_dma_fence_work(struct work_struct *pwork)
  */
 static int
 kbase_dma_fence_add_callback(struct kbase_jd_atom *katom,
-			     struct fence *fence,
-			     fence_func_t callback)
+			     struct dma_fence *fence,
+			     dma_fence_func_t callback)
 {
 	int err = 0;
 	struct kbase_dma_fence_cb *kbase_fence_cb;
@@ -343,7 +343,7 @@ kbase_dma_fence_add_callback(struct kbase_jd_atom *katom,
 	kbase_fence_cb->katom = katom;
 	INIT_LIST_HEAD(&kbase_fence_cb->node);
 
-	err = fence_add_callback(fence, &kbase_fence_cb->fence_cb, callback);
+	err = dma_fence_add_callback(fence, &kbase_fence_cb->fence_cb, callback);
 	if (err == -ENOENT) {
 		/* Fence signaled, clear the error and return */
 		err = 0;
@@ -356,7 +356,7 @@ kbase_dma_fence_add_callback(struct kbase_jd_atom *katom,
 		 * Get reference to fence that will be kept until callback gets
 		 * cleaned up in kbase_dma_fence_free_callbacks().
 		 */
-		fence_get(fence);
+		dma_fence_get(fence);
 		atomic_inc(&katom->dma_fence.dep_count);
 		/* Add callback to katom's list of callbacks */
 		list_add(&kbase_fence_cb->node, &katom->dma_fence.callbacks);
@@ -366,7 +366,7 @@ kbase_dma_fence_add_callback(struct kbase_jd_atom *katom,
 }
 
 static void
-kbase_dma_fence_cb(struct fence *fence, struct fence_cb *cb)
+kbase_dma_fence_cb(struct dma_fence *fence, struct dma_fence_cb *cb)
 {
 	struct kbase_dma_fence_cb *kcb = container_of(cb,
 				struct kbase_dma_fence_cb,
@@ -386,8 +386,8 @@ kbase_dma_fence_add_reservation_callback(struct kbase_jd_atom *katom,
 					 struct reservation_object *resv,
 					 bool exclusive)
 {
-	struct fence *excl_fence = NULL;
-	struct fence **shared_fences = NULL;
+	struct dma_fence *excl_fence = NULL;
+	struct dma_fence **shared_fences = NULL;
 	unsigned int shared_count = 0;
 	int err, i;
 
@@ -408,7 +408,7 @@ kbase_dma_fence_add_reservation_callback(struct kbase_jd_atom *katom,
 		 * and it's the fence's owner is responsible for singling the fence
 		 * before allowing it to disappear.
 		 */
-		fence_put(excl_fence);
+		dma_fence_put(excl_fence);
 
 		if (err)
 			goto out;
@@ -431,7 +431,7 @@ kbase_dma_fence_add_reservation_callback(struct kbase_jd_atom *katom,
 	 */
 out:
 	for (i = 0; i < shared_count; i++)
-		fence_put(shared_fences[i]);
+		dma_fence_put(shared_fences[i]);
 	kfree(shared_fences);
 
 	if (err) {
@@ -468,7 +468,7 @@ int kbase_dma_fence_wait(struct kbase_jd_atom *katom,
 			 struct kbase_dma_fence_resv_info *info)
 {
 	int err, i;
-	struct fence *fence;
+	struct dma_fence *fence;
 	struct ww_acquire_ctx ww_ctx;
 
 	lockdep_assert_held(&katom->kctx->jctx.lock);
@@ -490,7 +490,7 @@ int kbase_dma_fence_wait(struct kbase_jd_atom *katom,
 		dev_err(katom->kctx->kbdev->dev,
 			"Error %d locking reservations.\n", err);
 		atomic_set(&katom->dma_fence.dep_count, -1);
-		fence_put(fence);
+		dma_fence_put(fence);
 		return err;
 	}
 
@@ -580,8 +580,8 @@ void kbase_dma_fence_signal(struct kbase_jd_atom *katom)
 	KBASE_DEBUG_ASSERT(atomic_read(&katom->dma_fence.dep_count) == -1);
 
 	/* Signal the atom's fence. */
-	fence_signal(katom->dma_fence.fence);
-	fence_put(katom->dma_fence.fence);
+	dma_fence_signal(katom->dma_fence.fence);
+	dma_fence_put(katom->dma_fence.fence);
 	katom->dma_fence.fence = NULL;
 
 	kbase_dma_fence_free_callbacks(katom, false);
diff --git a/drivers/gpu/arm/midgard/mali_kbase_dma_fence.h b/drivers/gpu/arm/midgard/mali_kbase_dma_fence.h
index 3b0a69b..0fdaf26 100755
--- a/drivers/gpu/arm/midgard/mali_kbase_dma_fence.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_dma_fence.h
@@ -20,7 +20,7 @@
 
 #ifdef CONFIG_MALI_DMA_FENCE
 
-#include <linux/fence.h>
+#include <linux/dma-fence.h>
 #include <linux/list.h>
 #include <linux/reservation.h>
 
@@ -37,9 +37,9 @@ struct kbase_context;
  * @node:     List head for linking this callback to the katom
  */
 struct kbase_dma_fence_cb {
-	struct fence_cb fence_cb;
+	struct dma_fence_cb fence_cb;
 	struct kbase_jd_atom *katom;
-	struct fence *fence;
+	struct dma_fence *fence;
 	struct list_head node;
 };
 
diff --git a/drivers/gpu/arm/midgard/mali_kbase_jd.c b/drivers/gpu/arm/midgard/mali_kbase_jd.c
index 81952e2..0b0d9ed 100755
--- a/drivers/gpu/arm/midgard/mali_kbase_jd.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_jd.c
@@ -957,7 +957,7 @@ bool jd_submit_atom(struct kbase_context *kctx, const struct base_jd_atom_v2 *us
 	 * the scheduler: 'not ready to run' and 'dependency-only' jobs. */
 	jctx->job_nr++;
 
-	katom->start_timestamp.tv64 = 0;
+	katom->start_timestamp = 0;
 	katom->time_spent_us = 0;
 	katom->udata = user_atom->udata;
 	katom->kctx = kctx;
@@ -1830,7 +1830,7 @@ int kbase_jd_init(struct kbase_context *kctx)
 		kctx->jctx.atoms[i].status = KBASE_JD_ATOM_STATE_UNUSED;
 
 #ifdef CONFIG_MALI_DMA_FENCE
-		kctx->jctx.atoms[i].dma_fence.context = fence_context_alloc(1);
+		kctx->jctx.atoms[i].dma_fence.context = dma_fence_context_alloc(1);
 		atomic_set(&kctx->jctx.atoms[i].dma_fence.seqno, 0);
 		INIT_LIST_HEAD(&kctx->jctx.atoms[i].dma_fence.callbacks);
 #endif
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mem.c b/drivers/gpu/arm/midgard/mali_kbase_mem.c
index 60d31ac..999af01 100755
--- a/drivers/gpu/arm/midgard/mali_kbase_mem.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_mem.c
@@ -2037,19 +2037,11 @@ static int kbase_jd_user_buf_map(struct kbase_context *kctx,
 
 	pages = alloc->imported.user_buf.pages;
 
-#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 6, 0)
-	pinned_pages = get_user_pages(NULL, mm,
-			address,
-			alloc->imported.user_buf.nr_pages,
-			reg->flags & KBASE_REG_GPU_WR,
-			0, pages, NULL);
-#else
 	pinned_pages = get_user_pages_remote(NULL, mm,
 			address,
 			alloc->imported.user_buf.nr_pages,
-			reg->flags & KBASE_REG_GPU_WR,
-			0, pages, NULL);
-#endif
+			reg->flags & KBASE_REG_GPU_WR ? FOLL_WRITE : 0,
+			pages, NULL, NULL);
 
 	if (pinned_pages <= 0)
 		return pinned_pages;
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mem_linux.c b/drivers/gpu/arm/midgard/mali_kbase_mem_linux.c
index 1f5b3a1..308c756 100755
--- a/drivers/gpu/arm/midgard/mali_kbase_mem_linux.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_mem_linux.c
@@ -1189,13 +1189,11 @@ static struct kbase_va_region *kbase_mem_from_user_buffer(
 	/* We can't really store the page list because that would involve */
 	/* keeping the pages pinned - instead we pin/unpin around the job */
 	/* (as part of the external resources handling code) */
-#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 6, 0)
-	faulted_pages = get_user_pages(current, current->mm, address, *va_pages,
-			reg->flags & KBASE_REG_GPU_WR, 0, NULL, NULL);
-#else
+
 	faulted_pages = get_user_pages(address, *va_pages,
-			reg->flags & KBASE_REG_GPU_WR, 0, NULL, NULL);
-#endif
+			reg->flags & KBASE_REG_GPU_WR ? FOLL_WRITE : 0,
+			NULL, NULL);
+
 	up_read(&current->mm->mmap_sem);
 
 	if (faulted_pages != *va_pages)
@@ -1900,7 +1898,7 @@ static vm_fault_t kbase_cpu_vm_fault(struct vm_fault *vmf)
 
 	/* we don't use vmf->pgoff as it's affected by our mmap with
 	 * offset being a GPU VA or a cookie */
-	rel_pgoff = ((unsigned long)vmf->virtual_address - map->vm_start)
+	rel_pgoff = (vmf->address - map->vm_start)
 			>> PAGE_SHIFT;
 
 	kbase_gpu_vm_lock(map->kctx);
-- 
2.7.4

